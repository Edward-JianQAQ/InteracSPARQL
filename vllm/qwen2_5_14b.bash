vllm serve Qwen/Qwen2.5-14B-Instruct --port 4999 --max-num-seqs 1 --max_num_batched_tokens 25000 --max_model_len 25000 --gpu_memory_utilization 0.9 --dtype bfloat16